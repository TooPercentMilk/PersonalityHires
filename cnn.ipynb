{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CNN for Fake News Detection</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the implementation of a CNN that performs sentiment analysis on political statements. I used some of the code from https://cezannec.github.io/CNN_Text_Classification/. The steps taken to build this model are:\n",
    "\n",
    "<ol>\n",
    "    <li> Data Preprocessing\n",
    "    <li> Tokenizing Political Statements\n",
    "    <li> Train/Validation/Test Splitting (already set in the data folder)\n",
    "    <li> Defining a CNN for Sentiment Analysis\n",
    "    <li> Training and Evaluating the Model \n",
    "\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code comes from preprocessing.ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "numeric_labels = {'pants-fire':0, 'false':1, 'barely-true':2, 'half-true':3, 'mostly-true':4, 'true':5}\n",
    "path = os.getcwd() + '/data'\n",
    "headers = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'affiliation', 'barely_true',\n",
    "           'false', 'half_true', 'mostly_true', 'pants-fire', 'context']\n",
    "train = pd.read_csv(path + '/train.tsv', sep='\\t', header=None, names=headers)\n",
    "valid = pd.read_csv(path + '/valid.tsv', sep='\\t', header=None, names=headers)\n",
    "test = pd.read_csv(path + '/test.tsv', sep='\\t', header=None, names=headers)\n",
    "\n",
    "# lowercase, remove punctuation, remove numbers\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = text.lower()\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def clean_labels(label):\n",
    "    return numeric_labels[label]\n",
    "\n",
    "# cols indicates which columns we want to KEEP, the rest are dropped\n",
    "def drop_columns(df, cols):\n",
    "    new_df = df[cols]\n",
    "    new_df = new_df.dropna() # rows with incomplete information (NaN's) are dropped\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we will only be using the statement text and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>says the annies list political group supports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>when did the decline of coal start it started ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>hillary clinton agrees with john mccain by vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>the economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          statement\n",
       "0      1  says the annies list political group supports ...\n",
       "1      3  when did the decline of coal start it started ...\n",
       "2      4  hillary clinton agrees with john mccain by vot...\n",
       "3      1  health care reform legislation is likely to ma...\n",
       "4      3  the economic turnaround started at the end of ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['statement'] = train['statement'].apply(clean_text)\n",
    "train['label'] = train['label'].apply(clean_labels)\n",
    "train = drop_columns(train, ['label', 'statement'])\n",
    "train['statement'].to_csv('text_only.csv', index=False, header=['text'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>we have less americans working now than in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>when obama was sworn into office he did not us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>says having organizations parading as being so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>says nearly half of oregons children are poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>on attacks by republicans that various program...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          statement\n",
       "0      2  we have less americans working now than in the...\n",
       "1      0  when obama was sworn into office he did not us...\n",
       "2      1  says having organizations parading as being so...\n",
       "3      3      says nearly half of oregons children are poor\n",
       "4      3  on attacks by republicans that various program..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid['statement'] = valid['statement'].apply(clean_text)\n",
    "valid['label'] = valid['label'].apply(clean_labels)\n",
    "valid = drop_columns(valid, ['label', 'statement'])\n",
    "valid['statement'].to_csv('text_only.csv', index=False, header=['text'])\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>building a wall on the usmexico border will ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wisconsin is on pace to double the number of l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>says john mccain has done nothing to help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>suzanne bonamici supports a plan that will cut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>when asked by a reporter whether hes at the ce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          statement\n",
       "0      5  building a wall on the usmexico border will ta...\n",
       "1      1  wisconsin is on pace to double the number of l...\n",
       "2      1  says john mccain has done nothing to help the ...\n",
       "3      3  suzanne bonamici supports a plan that will cut...\n",
       "4      0  when asked by a reporter whether hes at the ce..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['statement'] = test['statement'].apply(clean_text)\n",
    "test['label'] = test['label'].apply(clean_labels)\n",
    "test = drop_columns(test, ['label', 'statement'])\n",
    "test['statement'].to_csv('text_only.csv', index=False, header=['text'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_train = train['statement']\n",
    "labels_train = np.array(train['label'])\n",
    "\n",
    "statements_valid = valid['statement']\n",
    "labels_valid = np.array(valid['label'])\n",
    "\n",
    "statements_test = test['statement']\n",
    "labels_test = np.array(test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is processed and ready to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenizing Political Statements</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will tokenize the political statements using a pretrained embedding model. Specifically, we will be using Google News word2vec model (https://github.com/eyaler/word2vec-slim/tree/master). According to the github README, \"the model was trained over a 3 billion word corpus, and contains 3 million words (of which ~930k are NOT phrases, i.e. do not contain underscores).\" Using this model will make tokenizing the statements much easier, as we will not need to create the token dictionaries by hand. There will be some words that are not listed in the pretrained embedded model, so we will account for that in the corresponding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# creating the pretrained embedding model\n",
    "embed = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all unlisted words\n",
    "def unlisted_words(embed, statements):\n",
    "    statement_words = [statement.split() for statement in statements]\n",
    "    unlisted_words = []\n",
    "    for statement in statement_words:\n",
    "        for word in statement:\n",
    "            try:\n",
    "                idx = embed.key_to_index.get(word)\n",
    "\n",
    "                # if word not in embedded list, add to unlisted words vec\n",
    "                if idx == None:\n",
    "                    unlisted_words.append(word)\n",
    "            except: \n",
    "                idx = 0\n",
    " \n",
    "    return unlisted_words\n",
    "\n",
    "# create a token dictionary for unlisted words \n",
    "def token_dict(unlisted, num):\n",
    "    t_dict = {}\n",
    "    i = 0\n",
    "    for word in unlisted:\n",
    "        t_dict[word] = num + i\n",
    "        i = i + 1\n",
    "    return t_dict\n",
    "\n",
    "# convert political statements to tokens\n",
    "def tokenize_all_statements(embed, statements, t_dict):\n",
    "    # split each statement into a list of words\n",
    "    statement_words = [statement.split() for statement in statements]\n",
    "    em_len = len(embed.key_to_index)\n",
    "\n",
    "    tokenized_statements = []\n",
    "    for statement in statement_words:\n",
    "        ints = []\n",
    "        for word in statement:\n",
    "            try:\n",
    "                idx = embed.key_to_index.get(word)\n",
    "\n",
    "                # if word not in embedded list, create new token\n",
    "                if idx == None:\n",
    "                    #idx = t_dict[word]\n",
    "                    idx = 0\n",
    "            except: \n",
    "                idx = 0\n",
    "            ints.append(idx)\n",
    "        tokenized_statements.append(ints)\n",
    "    \n",
    "    return tokenized_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "says the annies list political group supports thirdtrimester abortions on demand\n",
      "[109, 9, 0, 680, 424, 215, 2876, 0, 11132, 4, 656]\n",
      "we have less americans working now than in the 70s\n",
      "[34, 19, 350, 69404, 322, 92, 55, 0, 9, 0]\n",
      "building a wall on the usmexico border will take literally years\n",
      "[446, 0, 2270, 4, 9, 0, 1473, 21, 135, 5220, 72]\n"
     ]
    }
   ],
   "source": [
    "# find unlisted words and build token dictionary\n",
    "#unlisted = []\n",
    "\n",
    "#train_unlisted = unlisted_words(embed, statements_train)\n",
    "#valid_unlisted = unlisted_words(embed, statements_valid)\n",
    "#test_unlisted = unlisted_words(embed, statements_test)\n",
    "\n",
    "#unlisted.extend(x for x in train_unlisted if x not in unlisted)\n",
    "#unlisted.extend(x for x in valid_unlisted if x not in unlisted)\n",
    "#unlisted.extend(x for x in test_unlisted if x not in unlisted)\n",
    "\n",
    "num_tokens = len(embed.key_to_index)\n",
    "\n",
    "t_dict = token_dict({}, num_tokens)\n",
    "\n",
    "# tokenize the statements\n",
    "tokenized_train = tokenize_all_statements(embed, statements_train, t_dict)\n",
    "tokenized_valid = tokenize_all_statements(embed, statements_valid, t_dict)\n",
    "tokenized_test = tokenize_all_statements(embed, statements_test, t_dict)\n",
    "\n",
    "# check if the tokenizing works\n",
    "print(statements_train[0])\n",
    "print(tokenized_train[0])\n",
    "\n",
    "print(statements_valid[0])\n",
    "print(tokenized_valid[0])\n",
    "\n",
    "print(statements_test[0])\n",
    "print(tokenized_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to pad the tokenized statements list to make all the statements the same length. The final array should be 2D, with as many rows as statements and as many columns as the longest statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the features into a 2D representation\n",
    "def pad_features(tokenized_statements, max_length):\n",
    "\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(tokenized_statements), max_length), dtype=int)\n",
    "    \n",
    "    for i, row in enumerate(tokenized_statements):\n",
    "        features[i, -len(row):] = np.array(row)[:max_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_len_train = max(Counter([len(x.split()) for x in statements_train]))\n",
    "max_len_valid = max(Counter([len(x.split()) for x in statements_valid]))\n",
    "max_len_test = max(Counter([len(x.split()) for x in statements_test]))\n",
    "\n",
    "max_len = max(max_len_train, max_len_test, max_len_valid)\n",
    "\n",
    "features_train = np.array(pad_features(tokenized_train, max_len))\n",
    "features_valid = np.array(pad_features(tokenized_valid, max_len))\n",
    "features_test = np.array(pad_features(tokenized_test, max_len))\n",
    "\n",
    "# test statements to make sure dimensions are set\n",
    "assert len(features_train)==len(tokenized_train), \"Features should have as many rows as statements.\"\n",
    "assert len(features_train[0])==max_len, \"Each feature row should contain max_length values.\"\n",
    "assert len(features_valid)==len(tokenized_valid), \"Features should have as many rows as statements.\"\n",
    "assert len(features_valid[0])==max_len, \"Each feature row should contain max_length values.\"\n",
    "assert len(features_test)==len(tokenized_test), \"Features should have as many rows as statements.\"\n",
    "assert len(features_test[0])==max_len, \"Each feature row should contain max_length values.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the training data is tokenized and put into a 2D array. We repeat the same thing for test.tsv and valid.tsv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train/Validation/Test Splitting (already set in the data folder)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data folder already has the data split and 2D arrays have been made for the train, valid, and test sets. Now, we use data loaders and batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Defining a CNN for Sentiment Analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FakeNewsCNN(nn.Module):    \n",
    "    def __init__(self, embed_model, embedding_dim, max_features, num_filters=100):\n",
    "        super(FakeNewsCNN, self).__init__()\n",
    "        filter_sizes = [1,2,3,5]\n",
    "        num_filters = 36\n",
    "        n_classes = 6\n",
    "        self.embedding = nn.Embedding(max_features, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embed_model.vectors, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embedding_dim)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  \n",
    "        logit = self.fc1(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FakeNewsCNN setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNewsCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv2d(1, 36, kernel_size=(1, 300), stride=(1, 1))\n",
      "    (1): Conv2d(1, 36, kernel_size=(2, 300), stride=(1, 1))\n",
      "    (2): Conv2d(1, 36, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (3): Conv2d(1, 36, kernel_size=(5, 300), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc1): Linear(in_features=144, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# store pretrained vocab\n",
    "pretrained_words = []\n",
    "for word in embed.key_to_index:\n",
    "    pretrained_words.append(word)\n",
    "\n",
    "vocab_size =  len(pretrained_words)\n",
    " \n",
    "embedding_dim = len(embed[pretrained_words[0]])\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "model = FakeNewsCNN(embed, embedding_dim, vocab_size, num_filters=100)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(features_train), torch.from_numpy(labels_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(features_valid), torch.from_numpy(labels_valid))\n",
    "test_data = TensorDataset(torch.from_numpy(features_test), torch.from_numpy(labels_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# shuffling and batching data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeNewsCNN(\n",
       "  (embedding): Embedding(299567, 300)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 36, kernel_size=(1, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 36, kernel_size=(2, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 36, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (3): Conv2d(1, 36, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=144, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def init_weights(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train(net, train_loader, epochs, labels, print_every=100):\n",
    "\n",
    "\n",
    "    counter = 0 # for printing\n",
    "    \n",
    "    # train for some number of epochs\n",
    "    net.train()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            \n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output, labels)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 2, 4, 2, 0, 3, 5, 2, 1, 2, 1, 4, 2, 1, 1, 0, 3, 4, 3, 0, 4, 3, 2, 2,\n",
      "        3, 1, 3, 1, 1, 4, 2, 4, 3, 4, 1, 2, 5, 1, 5, 1, 4, 5, 0, 1, 0, 3, 4, 5,\n",
      "        4, 3])\n",
      "tensor([0, 3, 4, 3, 2, 4, 4, 2, 3, 3, 2, 1, 0, 3, 3, 2, 2, 2, 1, 1, 3, 1, 0, 3,\n",
      "        3, 3, 1, 5, 0, 5, 2, 4, 4, 1, 4, 4, 3, 2, 1, 0, 3, 4, 4, 0, 2, 3, 3, 4,\n",
      "        0, 3])\n",
      "tensor([1, 1, 2, 0, 1, 1, 4, 3, 3, 2, 1, 4, 1, 2, 1, 1, 3, 4, 4, 5, 3, 4, 4, 5,\n",
      "        2, 2, 0, 5, 0, 2, 4, 1, 5, 0, 4, 4, 5, 2, 3, 1, 4, 3, 2, 4, 2, 4, 0, 4,\n",
      "        1, 2])\n",
      "tensor([0, 2, 2, 5, 4, 0, 4, 2, 1, 3, 4, 3, 3, 3, 1, 0, 2, 1, 3, 5, 5, 1, 1, 4,\n",
      "        1, 2, 1, 3, 2, 3, 4, 1, 5, 0, 3, 1, 2, 4, 3, 1, 1, 4, 1, 4, 3, 4, 3, 0,\n",
      "        1, 2])\n",
      "tensor([3, 3, 3, 2, 3, 0, 5, 1, 0, 3, 1, 4, 0, 5, 1, 0, 1, 4, 3, 3, 4, 3, 3, 4,\n",
      "        2, 5, 1, 4, 2, 3, 2, 1, 3, 1, 2, 1, 4, 5, 2, 1, 5, 3, 3, 3, 2, 2, 3, 1,\n",
      "        1, 2])\n",
      "tensor([0, 2, 3, 3, 4, 1, 4, 1, 4, 0, 2, 1, 1, 1, 4, 2, 4, 3, 0, 1, 4, 3, 1, 5,\n",
      "        4, 5, 3, 0, 3, 3, 1, 0, 1, 2, 2, 2, 2, 3, 1, 4, 1, 5, 5, 3, 3, 0, 1, 0,\n",
      "        1, 5])\n",
      "tensor([1, 0, 3, 2, 1, 1, 1, 4, 2, 0, 1, 1, 2, 1, 5, 5, 1, 1, 3, 4, 0, 2, 5, 5,\n",
      "        1, 4, 3, 5, 1, 2, 1, 4, 0, 0, 1, 0, 1, 4, 3, 1, 1, 0, 2, 4, 4, 3, 2, 2,\n",
      "        2, 1])\n",
      "tensor([2, 2, 1, 1, 1, 0, 3, 5, 1, 0, 3, 3, 5, 5, 5, 3, 3, 1, 2, 0, 3, 2, 3, 5,\n",
      "        3, 1, 1, 5, 1, 5, 3, 3, 2, 1, 1, 3, 1, 4, 2, 2, 1, 4, 5, 1, 2, 5, 5, 4,\n",
      "        4, 2])\n",
      "tensor([1, 3, 4, 2, 4, 3, 2, 3, 4, 3, 2, 1, 2, 3, 1, 0, 2, 3, 4, 5, 3, 1, 4, 5,\n",
      "        1, 1, 5, 4, 5, 4, 2, 3, 4, 0, 4, 4, 4, 1, 1, 4, 3, 2, 1, 1, 5, 1, 1, 0,\n",
      "        2, 0])\n",
      "tensor([3, 2, 5, 3, 2, 0, 4, 4, 2, 5, 3, 4, 4, 1, 3, 3, 3, 2, 5, 5, 0, 3, 2, 5,\n",
      "        3, 1, 3, 2, 4, 1, 2, 2, 3, 3, 2, 3, 3, 4, 1, 1, 0, 4, 2, 2, 3, 0, 3, 3,\n",
      "        2, 5])\n",
      "tensor([4, 3, 0, 4, 2, 1, 5, 1, 3, 4, 1, 3, 5, 3, 5, 3, 2, 5, 2, 5, 5, 0, 1, 2,\n",
      "        4, 0, 1, 0, 4, 1, 3, 0, 4, 3, 2, 4, 1, 1, 0, 1, 5, 3, 3, 3, 4, 1, 0, 0,\n",
      "        1, 3])\n",
      "tensor([2, 4, 3, 1, 3, 2, 1, 2, 5, 3, 2, 3, 0, 4, 4, 4, 3, 0, 1, 4, 5, 1, 1, 5,\n",
      "        5, 0, 3, 0, 1, 4, 0, 2, 1, 1, 3, 4, 1, 1, 5, 3, 4, 5, 5, 2, 2, 5, 5, 4,\n",
      "        2, 1])\n",
      "tensor([4, 3, 2, 1, 5, 5, 2, 2, 3, 5, 5, 3, 5, 3, 2, 2, 0, 5, 2, 1, 2, 3, 3, 1,\n",
      "        1, 1, 2, 3, 1, 4, 4, 3, 5, 4, 3, 2, 1, 1, 5, 1, 2, 3, 3, 5, 2, 3, 5, 1,\n",
      "        0, 4])\n",
      "tensor([4, 3, 2, 4, 5, 3, 3, 0, 3, 1, 1, 1, 5, 3, 5, 0, 1, 1, 3, 0, 1, 2, 2, 4,\n",
      "        0, 3, 5, 2, 1, 3, 4, 4, 2, 5, 3, 0, 0, 5, 5, 1, 1, 2, 0, 4, 0, 0, 0, 5,\n",
      "        3, 4])\n",
      "tensor([5, 0, 2, 1, 0, 5, 3, 2, 4, 1, 3, 4, 4, 5, 4, 2, 4, 2, 1, 1, 4, 1, 3, 3,\n",
      "        3, 1, 0, 2, 1, 1, 3, 2, 1, 1, 3, 5, 2, 2, 3, 2, 0, 3, 3, 1, 2, 1, 4, 5,\n",
      "        4, 4])\n",
      "tensor([4, 3, 4, 4, 4, 1, 2, 2, 4, 5, 1, 5, 1, 2, 1, 3, 3, 2, 5, 3, 1, 1, 3, 2,\n",
      "        4, 4, 2, 4, 4, 1, 3, 4, 2, 4, 0, 2, 3, 4, 2, 1, 0, 3, 1, 1, 1, 3, 4, 3,\n",
      "        1, 5])\n",
      "tensor([3, 2, 4, 2, 1, 4, 1, 1, 2, 4, 5, 2, 0, 2, 1, 1, 2, 4, 2, 2, 2, 4, 3, 5,\n",
      "        0, 3, 1, 4, 4, 5, 2, 1, 5, 4, 5, 4, 3, 1, 4, 4, 2, 2, 1, 2, 0, 2, 3, 0,\n",
      "        4, 4])\n",
      "tensor([2, 2, 1, 0, 1, 3, 2, 1, 1, 2, 4, 4, 5, 0, 4, 4, 2, 2, 1, 2, 4, 1, 4, 2,\n",
      "        5, 3, 4, 4, 5, 4, 2, 1, 2, 5, 2, 1, 4, 5, 0, 3, 4, 1, 2, 2, 3, 2, 2, 2,\n",
      "        5, 4])\n",
      "tensor([2, 4, 3, 5, 5, 4, 3, 1, 4, 1, 3, 4, 1, 5, 1, 5, 2, 1, 3, 5, 4, 4, 1, 3,\n",
      "        1, 1, 2, 2, 0, 3, 2, 1, 1, 3, 3, 5, 4, 5, 4, 3, 0, 3, 2, 5, 2, 5, 1, 3,\n",
      "        3, 2])\n",
      "tensor([1, 5, 1, 0, 1, 0, 1, 1, 3, 5, 3, 0, 3, 2, 4, 4, 2, 2, 2, 3, 3, 2, 1, 2,\n",
      "        0, 5, 5, 1, 3, 5, 2, 4, 5, 4, 1, 5, 2, 0, 5, 4, 2, 2, 3, 5, 4, 1, 2, 2,\n",
      "        3, 2])\n",
      "tensor([4, 5, 0, 2, 1, 1, 4, 3, 4, 2, 5, 3, 4, 3, 4, 3, 1, 4, 2, 5, 5, 1, 0, 3,\n",
      "        5, 4, 4, 4, 1, 1, 0, 3, 5, 2, 2, 0, 1, 5, 4, 1, 3, 3, 4, 5, 2, 3, 4, 2,\n",
      "        4, 4])\n",
      "tensor([4, 1, 3, 4, 4, 5, 5, 4, 5, 0, 4, 1, 5, 3, 1, 1, 2, 2, 4, 2, 3, 0, 2, 0,\n",
      "        3, 5, 2, 3, 3, 4, 3, 2, 5, 1, 4, 3, 2, 2, 2, 4, 5, 3, 2, 4, 0, 3, 5, 1,\n",
      "        4, 0])\n",
      "tensor([0, 3, 2, 3, 3, 1, 1, 1, 1, 4, 4, 2, 4, 3, 2, 4, 1, 1, 2, 4, 4, 4, 1, 4,\n",
      "        1, 2, 2, 4, 4, 5, 2, 4, 3, 2, 3, 4, 1, 4, 4, 1, 2, 1, 1, 4, 4, 5, 3, 2,\n",
      "        2, 5])\n",
      "tensor([5, 5, 3, 3, 4, 2, 3, 5, 4, 4, 5, 0, 2, 1, 2, 1, 1, 1, 2, 4, 3, 1, 3, 4,\n",
      "        3, 2, 4, 0, 5, 3, 3, 5, 2, 4, 2, 1, 3, 4, 1, 0, 4, 4, 3, 5, 4, 5, 1, 4,\n",
      "        4, 3])\n",
      "tensor([4, 4, 4, 2, 1, 5, 3, 2, 0, 5, 4, 0, 4, 5, 5, 3, 1, 3, 4, 4, 3, 2, 2, 3,\n",
      "        4, 4, 2, 2, 0, 4, 5, 0, 3, 5, 4, 2, 5, 4, 3, 5, 1, 4, 3, 5, 4, 3, 5, 5,\n",
      "        4, 0])\n",
      "tensor([5, 5, 2, 5, 1, 3, 2, 2, 1, 4, 0, 1, 1, 4, 3, 1, 1, 4, 3, 5, 1, 5, 5, 3,\n",
      "        1, 2, 5, 1, 4, 4, 2, 1, 4, 0])\n",
      "Epoch: 1/10... Step: 100... Loss: 1.477917... Val Loss: 1.727653\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      2\u001b[0m print_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, epochs, labels, print_every)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# calculate the loss and perform backprop\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# loss stats\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CNN/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CNN/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "print_every = 100\n",
    "\n",
    "train(model, train_loader, epochs, labels_train, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
