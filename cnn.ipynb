{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CNN for Fake News Detection</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the implementation of a CNN that performs sentiment analysis on political statements. I used some of the code from https://cezannec.github.io/CNN_Text_Classification/. The steps taken to build this model are:\n",
    "\n",
    "<ol>\n",
    "    <li> Data Preprocessing\n",
    "    <li> Tokenizing Political Statements\n",
    "    <li> Train/Validation/Test Splitting (already set in the data folder)\n",
    "    <li> Defining a CNN for Sentiment Analysis\n",
    "    <li> Training and Evaluating the Model \n",
    "\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code comes from preprocessing.ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "numeric_labels = {'pants-fire':0, 'false':1, 'barely-true':2, 'half-true':3, 'mostly-true':4, 'true':5}\n",
    "path = os.getcwd() + '/data'\n",
    "headers = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'affiliation', 'barely_true',\n",
    "           'false', 'half_true', 'mostly_true', 'pants-fire', 'context']\n",
    "train = pd.read_csv(path + '/train.tsv', sep='\\t', header=None, names=headers)\n",
    "valid = pd.read_csv(path + '/valid.tsv', sep='\\t')\n",
    "\n",
    "# lowercase, remove punctuation, remove numbers\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = text.lower()\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def clean_labels(label):\n",
    "    return numeric_labels[label]\n",
    "\n",
    "# cols indicates which columns we want to KEEP, the rest are dropped\n",
    "def drop_columns(df, cols):\n",
    "    new_df = df[cols]\n",
    "    new_df = new_df.dropna() # rows with incomplete information (NaN's) are dropped\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we will only be using the statement text and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>says the annies list political group supports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>when did the decline of coal start it started ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>hillary clinton agrees with john mccain by vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>the economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          statement\n",
       "0      1  says the annies list political group supports ...\n",
       "1      3  when did the decline of coal start it started ...\n",
       "2      4  hillary clinton agrees with john mccain by vot...\n",
       "3      1  health care reform legislation is likely to ma...\n",
       "4      3  the economic turnaround started at the end of ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['statement'] = train['statement'].apply(clean_text)\n",
    "train['label'] = train['label'].apply(clean_labels)\n",
    "train = drop_columns(train, ['label', 'statement'])\n",
    "train['statement'].to_csv('text_only.csv', index=False, header=['text'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = train['statement']\n",
    "labels = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is processed and ready to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenizing Political Statements</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will tokenize the political statements using a pretrained embedding model. Specifically, we will be using Google News word2vec model (https://github.com/eyaler/word2vec-slim/tree/master). According to the github README, \"the model was trained over a 3 billion word corpus, and contains 3 million words (of which ~930k are NOT phrases, i.e. do not contain underscores).\" Using this model will make tokenizing the statements much easier, as we will not need to create the token dictionaries by hand. There will be some words that are not listed in the pretrained embedded model, so we will account for that in the corresponding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# creating the pretrained embedding model\n",
    "embed = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert political statements to tokens\n",
    "def tokenize_all_statements(embed, statements):\n",
    "    # split each statement into a list of words\n",
    "    statement_words = [statement.split() for statement in statements]\n",
    "    em_len = len(embed.key_to_index)\n",
    "    i = 0\n",
    "\n",
    "    tokenized_statements = []\n",
    "    for statement in statement_words:\n",
    "        ints = []\n",
    "        for word in statement:\n",
    "            try:\n",
    "                idx = embed.key_to_index.get(word)\n",
    "\n",
    "                # if word not in embedded list, create new token\n",
    "                if idx == None:\n",
    "                    idx = em_len + i\n",
    "                    i = i + 1\n",
    "            except: \n",
    "                idx = 0\n",
    "            ints.append(idx)\n",
    "        tokenized_statements.append(ints)\n",
    "    \n",
    "    return tokenized_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "says the annies list political group supports thirdtrimester abortions on demand\n",
      "[109, 9, 299567, 680, 424, 215, 2876, 299568, 11132, 4, 656]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the statements\n",
    "tokenized_states = tokenize_all_statements(embed, statements)\n",
    "\n",
    "# check if the tokenizing works\n",
    "print(statements[0])\n",
    "print(tokenized_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to pad the tokenized statements list to make all the statements the same length. The final array should be 2D, with as many rows as statements and as many columns as the longest review."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
